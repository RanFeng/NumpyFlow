{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e726bcb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Operation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c4c09bed5b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Operation' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Set, Type, Union\n",
    "\n",
    "class Mul(Operation) :\n",
    "    def __call__(self, x, y):\n",
    "        self.var = (x, y)\n",
    "        out = x.data * y.data\n",
    "        return out\n",
    "    def backward(self, grad):\n",
    "        x, y = self.var\n",
    "        x.backward(grad * y.data)\n",
    "        y.backward(grad * x.data)\n",
    "\n",
    "class Add(Operation) :\n",
    "    def __call__(self, x, y):\n",
    "        self.var = (x, y)\n",
    "        out = x.data + y.data\n",
    "        return out\n",
    "    def backward(self, grad):\n",
    "        x, y = self.var\n",
    "        x.backward(grad)\n",
    "        y.backward(grad)\n",
    "\n",
    "class Tensor :\n",
    "    def __init__(self, data=None, *, requires_grad=False, creator=None):\n",
    "        assert isinstance(requires_grad, bool)\n",
    "        assert isinstance(creator, (Operation, None.__class__))\n",
    "        self.data = None\n",
    "        if isinstance(data, (int, float, bool)):\n",
    "            data = [data]\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            data = np.array(data)\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.data = data.copy()\n",
    "        elif isinstance(data, Tensor):\n",
    "            raise ValueError(\"输入的是 Tensor\")\n",
    "        else:\n",
    "            raise ValueError(\"输入类型未知\", type(data), data)\n",
    "        if creator is None:\n",
    "            creator = Assign()\n",
    "            creator(self)\n",
    "        self.creator = creator\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        \n",
    "    \n",
    "    def op(self, Op:Type[Add], input_vars):\n",
    "        if Op == None:\n",
    "            return \n",
    "        tensor_vars = tuple(\n",
    "            Tensor(var) if not isinstance(var, Tensor) else var for var in input_vars\n",
    "        )\n",
    "        f = Op()\n",
    "        op_out = f(*tensor_vars)\n",
    "        return Tensor(op_out, creator=f)\n",
    "        \n",
    "    \n",
    "    def __mul__(self, other):  # 乘法\n",
    "        return self.op(Mul, (self, other))\n",
    "    def __rmul__(self, other): # 乘法\n",
    "        return self.op(Mul, (other, self))\n",
    "    \n",
    "    def __add__(self, other):  # 加法\n",
    "        return self.op(Add, (self, other))\n",
    "    def __radd__(self, other): # 加法\n",
    "        return self.op(Add, (other, self))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tensor with shape: {}\\n{}\".format(self.shape, self.data)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def broadcastable(self, grad, ashape):\n",
    "        \"\"\"\n",
    "        保证传递的梯度shape一致，用于兼容广播机制的反向传播\n",
    "        :param grad:\n",
    "        :param ashape:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if grad.shape == ashape:\n",
    "            return grad\n",
    "        grad_bak = grad.sum(axis=tuple(range(grad.ndim - len(ashape))))\n",
    "        keepdims = tuple(n for (n, i) in enumerate(grad_bak.shape) if i != ashape[n])\n",
    "        if keepdims:\n",
    "            grad_bak = grad_bak.sum(axis=keepdims, keepdims=True)\n",
    "        return grad_bak\n",
    "    \n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self.data, dtype=np.float64)\n",
    "        try:    \n",
    "            self.grad += grad\n",
    "        except ValueError:  # self.grad.shape 长度不等于 grad.shape，用于适应广播机制\n",
    "            grad = self.broadcastable(grad, self.grad.shape)\n",
    "            self.grad += grad\n",
    "        self.grad += grad\n",
    "        if self.creator:\n",
    "            self.creator.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db03840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def fun(a, b ,*,c):\n",
    "    print(a, b, c)\n",
    "\n",
    "fun(1, 2, c = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3c4b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with shape: (1, 2)\n",
      "[[2 4]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([[2, 4]])\n",
    "d = Tensor(1)\n",
    "e = a * d\n",
    "print(e)\n",
    "# e.backward(1)\n",
    "# print(a.grad, d.grad)\n",
    "# print(Tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0a5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nf import Tensor\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import *\n",
    "from time import time\n",
    "\n",
    "\n",
    "def func(x,y,z):\n",
    "    f0 = (x[1,0].T * y[0,1].T).T * z * x\n",
    "    f1 = f0 * (x + y + z) * y * y * y * (y+z) #! 有错[9,23,29]\n",
    "    f2 = y[0,3] + x[0,2]\n",
    "    f3 = y * y - z\n",
    "    f4 = z - x\n",
    "    f5 = -x.flatten() + y.flatten() - (x*z).flatten() * 2.0\n",
    "    f6 = f1[1,3] + f1[0,3] * f2 - z[0,1] ** 2.2\n",
    "    f7 = f3 + f4 + f6\n",
    "    f8 = f7 - f3 + f4 * 3.6\n",
    "    f9 = f8.flatten() / f5 + f7.flatten()\n",
    "    f10 = -f9 * f5\n",
    "    f11 = ((x*z) @ x.transpose(3, 4) @ y.permute(0,4,2,3,1)).transpose(0,4)\n",
    "    f12 = f11.transpose(3,4).flatten() * 5.0 ** x.transpose(1,4).flatten() / y.flatten() * (x/z).flatten() + 2.0\n",
    "    f13 = f10.reshape(f11.shape) * f11 / f12.reshape(f11.shape)\n",
    "    f14 = (x.transpose(3,4) @ y).permute(0,2,4,3,1) @ f13.permute(4,2,0,1,3)\n",
    "    f15 = f14.sum() * f14.mean((0,2))\n",
    "    return f15\n",
    "\n",
    "def th_grad_Test(x,y,z):\n",
    "    x = Variable(torch.from_numpy(x), requires_grad=True)\n",
    "    y = Variable(torch.from_numpy(y), requires_grad=True)\n",
    "    z = Variable(torch.from_numpy(z), requires_grad=True)\n",
    "    t1 = time()\n",
    "    f9 = func(x, y, z)\n",
    "    t2 = time() - t1\n",
    "    t1 = time()\n",
    "    f9.backward(torch.ones_like(f9), retain_graph=True)\n",
    "    print(\"th\", t2, time() - t1)\n",
    "\n",
    "    return [x.grad.numpy(), y.grad.numpy(), z.grad.numpy()]\n",
    "\n",
    "\n",
    "def nf_grad_Test(x,y,z):\n",
    "    x = Tensor(x, requires_grad=True)\n",
    "    y = Tensor(y, requires_grad=True)\n",
    "    z = Tensor(z, requires_grad=True)\n",
    "    t1 = time()\n",
    "    f9 = func(x,y,z)\n",
    "    t2 = time() - t1\n",
    "    t1 = time()\n",
    "    f9.backward()\n",
    "    print(\"nf\", t2, time() - t1)\n",
    "    return [x.grad, y.grad, z.grad]\n",
    "\n",
    "def test1():\n",
    "    np.random.seed(28)\n",
    "    x = np.random.random([2,4,6,3,4])\n",
    "    y = np.random.random([2,4,6,3,4])\n",
    "    z = np.random.random([2,4,1,1,4])\n",
    "\n",
    "    grad_th = th_grad_Test(x,y,z)\n",
    "    grad_nf = nf_grad_Test(x,y,z)\n",
    "\n",
    "    for (thi, nfi) in zip(grad_th, grad_nf):\n",
    "        a = np.allclose(nfi, thi)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf0af6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th 0.0018968582153320312 0.0019490718841552734\n",
      "nf 0.0017888545989990234 0.011336088180541992\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
